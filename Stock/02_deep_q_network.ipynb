{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Function\n",
    "* used to approximate the reward based on a state\n",
    "* Q(s,a) calculates the expected future value from state **s** and action **a**\n",
    "* in DQN, we use a **neural network to approximate the reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "* Environment\n",
    "* Agent\n",
    "* Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, act, days, percentage):\n",
    "        self.act = act\n",
    "        self.days = days\n",
    "        self.percentage = percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas_datareader as pdr\n",
    "import datetime\n",
    "\n",
    "BUY = 'buy'\n",
    "SELL = 'sell'\n",
    "SKIP = 'skip'\n",
    "\n",
    "class Environment:\n",
    "    \n",
    "    min_days_to_hold = 2\n",
    "    max_days_to_hold = 6\n",
    "    \n",
    "    def __init__(self, \n",
    "                 ticker, \n",
    "                 initial_deposit = 100000,\n",
    "                 from_date = datetime.datetime(2007, 1, 1), \n",
    "                 to_date = datetime.datetime(2017, 1, 1),\n",
    "                 window = 20):\n",
    "        self.initial_deposit = initial_deposit\n",
    "        self.window = window\n",
    "        self.data = pdr.get_data_google(ticker, from_date, to_date)\n",
    "        self.pct_data = self.data.pct_change().fillna(0)\n",
    "        self.data_length = len(self.data)\n",
    "        \n",
    "        actions = np.array([BUY, SELL, SKIP])\n",
    "        days_to_holds = np.arange(Environment.min_days_to_hold, \n",
    "                                  Environment.max_days_to_hold + 1,\n",
    "                                  1)\n",
    "        \n",
    "        self.action_space = [Action(act, days, 3) for act in actions for days in days_to_holds]\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.deposit = self.initial_deposit\n",
    "        self.current_index = self.window\n",
    "        self.actions = {}\n",
    "        \n",
    "        return self.state()\n",
    "    \n",
    "    def score(self):\n",
    "        return self.deposit\n",
    "    \n",
    "    def enough_data_provided(self):\n",
    "        return self.current_index + Environment.max_days_to_hold <= self.data_length\n",
    "    \n",
    "    def state(self):\n",
    "        return self.pct_data.iloc[self.current_index - self.window:self.current_index]['Close']\n",
    "    \n",
    "    # def price_state(self):\n",
    "    #    return self.data.iloc[self.current_index - self.window:self.current_index]['Close']\n",
    "        \n",
    "    def state_size(self):\n",
    "        return self.window\n",
    "    \n",
    "    def action_size(self):\n",
    "        return len(self.action_space)\n",
    "        \n",
    "    def step(self, action_idx: int):\n",
    "        action = self.action_space[action_idx]\n",
    "        #print('\\t=> current action is: {} at {}'.format(action, self.data.index[self.current_index]))\n",
    "        \n",
    "        df = self.data.iloc[self.current_index: self.current_index + action.days]\n",
    "        on_date = df.index[0]\n",
    "        first_day_price = df.iloc[0]['Close']\n",
    "        last_day_price = df.iloc[-1]['Close']\n",
    "        \n",
    "        if action.act == BUY:\n",
    "            reward = last_day_price - first_day_price\n",
    "        elif action.act == SELL:\n",
    "            reward = first_day_price - last_day_price\n",
    "        elif action.act == SKIP:\n",
    "            reward = 0\n",
    "            \n",
    "        self.actions[on_date] = (action, reward)\n",
    "        \n",
    "        self.current_index += action.days\n",
    "        self.deposit += reward * (self.deposit*action.percentage/100)\n",
    "        \n",
    "        next_state = self.state()\n",
    "        done = False\n",
    "        _ = None\n",
    "        return next_state, reward, done, _ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=mean_squared_error,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 15, state size: 20\n"
     ]
    }
   ],
   "source": [
    "env = Environment('AAPL')\n",
    "state_size = env.state_size()\n",
    "action_size = env.action_size()\n",
    "print('Action size: {}, state size: {}'.format(action_size, state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size)\n",
    "EPISODES = 500\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/500, score: 5822.160844845312, e: 1.0\n",
      "episode: 1/500, score: 47791.39638356209, e: 0.99\n",
      "episode: 2/500, score: 859137.4077476239, e: 0.98\n",
      "episode: 3/500, score: 48503.41733107561, e: 0.97\n",
      "episode: 4/500, score: 83329.94853865087, e: 0.96\n",
      "episode: 5/500, score: 75498.00394796302, e: 0.95\n",
      "episode: 6/500, score: 82316.16798564426, e: 0.94\n",
      "episode: 7/500, score: 7176.9721024450555, e: 0.93\n",
      "episode: 8/500, score: 232752.05095765693, e: 0.92\n",
      "episode: 9/500, score: 43505.46217392056, e: 0.91\n",
      "episode: 10/500, score: 41839.26145666254, e: 0.9\n",
      "episode: 11/500, score: 83453.2727221593, e: 0.9\n",
      "episode: 12/500, score: 68212.55678993631, e: 0.89\n",
      "episode: 13/500, score: 200235.52120640484, e: 0.88\n",
      "episode: 14/500, score: 54848.88173651335, e: 0.87\n",
      "episode: 15/500, score: 52226.68767160795, e: 0.86\n",
      "episode: 16/500, score: 9984.133457158414, e: 0.85\n",
      "episode: 17/500, score: 30235.0298211429, e: 0.84\n",
      "episode: 18/500, score: 112466.18758673618, e: 0.83\n",
      "episode: 19/500, score: 1602606.3689595526, e: 0.83\n",
      "episode: 20/500, score: 484754.8563956943, e: 0.82\n",
      "episode: 21/500, score: 13142.730069028363, e: 0.81\n",
      "episode: 22/500, score: 294673.4682846737, e: 0.8\n",
      "episode: 23/500, score: 333558.2967390254, e: 0.79\n",
      "episode: 24/500, score: 29355.017895411278, e: 0.79\n",
      "episode: 25/500, score: 93478.41964496774, e: 0.78\n",
      "episode: 26/500, score: 174861.7832473131, e: 0.77\n",
      "episode: 27/500, score: 34422.864282157105, e: 0.76\n",
      "episode: 28/500, score: 23458.500719394913, e: 0.75\n",
      "episode: 29/500, score: 53830.3116062871, e: 0.75\n",
      "episode: 30/500, score: 17934.2371412661, e: 0.74\n",
      "episode: 31/500, score: 52330.497854517926, e: 0.73\n",
      "episode: 32/500, score: 19563.22030975455, e: 0.72\n",
      "episode: 33/500, score: 471836.57339649496, e: 0.72\n",
      "episode: 34/500, score: 108854.02953333064, e: 0.71\n",
      "episode: 35/500, score: 39951.47579989542, e: 0.7\n",
      "episode: 36/500, score: 31090.3033697949, e: 0.7\n",
      "episode: 37/500, score: 14391.953652694256, e: 0.69\n",
      "episode: 38/500, score: 62952.55401636558, e: 0.68\n",
      "episode: 39/500, score: 42211.26412106537, e: 0.68\n",
      "episode: 40/500, score: 136194.04719791585, e: 0.67\n",
      "episode: 41/500, score: 40080.261747680895, e: 0.66\n",
      "episode: 42/500, score: 34614.30067234876, e: 0.66\n",
      "episode: 43/500, score: 19610.966585470407, e: 0.65\n",
      "episode: 44/500, score: 107330.62270853911, e: 0.64\n",
      "episode: 45/500, score: 17668.789213244545, e: 0.64\n",
      "episode: 46/500, score: 39249.43268469132, e: 0.63\n",
      "episode: 47/500, score: 59931.98254562393, e: 0.62\n",
      "episode: 48/500, score: 116776.57811859205, e: 0.62\n",
      "episode: 49/500, score: 395292.1079651191, e: 0.61\n",
      "episode: 50/500, score: 30428.172756923377, e: 0.61\n",
      "episode: 51/500, score: 77325.10657547315, e: 0.6\n",
      "episode: 52/500, score: 34237.75983363476, e: 0.59\n",
      "episode: 53/500, score: 85545.34948201185, e: 0.59\n",
      "episode: 54/500, score: 44091.605167851834, e: 0.58\n",
      "episode: 55/500, score: 8062.618158837223, e: 0.58\n",
      "episode: 56/500, score: 25842.761388789102, e: 0.57\n",
      "episode: 57/500, score: 51691.124844779566, e: 0.56\n",
      "episode: 58/500, score: 179276.31650524127, e: 0.56\n",
      "episode: 59/500, score: 274385.8026810516, e: 0.55\n",
      "episode: 60/500, score: 16486.353677032326, e: 0.55\n",
      "episode: 61/500, score: 347306.9299512157, e: 0.54\n",
      "episode: 62/500, score: 38120.35561081166, e: 0.54\n",
      "episode: 63/500, score: 32786.11460876587, e: 0.53\n",
      "episode: 64/500, score: 183664.72029478292, e: 0.53\n",
      "episode: 65/500, score: 66183.99102933444, e: 0.52\n",
      "episode: 66/500, score: 46574.326381327, e: 0.52\n",
      "episode: 67/500, score: 46033.200844349725, e: 0.51\n",
      "episode: 68/500, score: 38627.77061924154, e: 0.5\n",
      "episode: 69/500, score: 19911.322477648537, e: 0.5\n",
      "episode: 70/500, score: 102000.59063628635, e: 0.49\n",
      "episode: 71/500, score: 13767.124770059334, e: 0.49\n",
      "episode: 72/500, score: 149350.44228364574, e: 0.48\n",
      "episode: 73/500, score: 110251.79638895045, e: 0.48\n",
      "episode: 74/500, score: 126208.20432846785, e: 0.48\n",
      "episode: 75/500, score: 136147.8690567212, e: 0.47\n",
      "episode: 76/500, score: 17149.263992239292, e: 0.47\n",
      "episode: 77/500, score: 53096.987524630036, e: 0.46\n",
      "episode: 78/500, score: 83572.34675372153, e: 0.46\n",
      "episode: 79/500, score: 390974.07313256717, e: 0.45\n",
      "episode: 80/500, score: 19388.347053597256, e: 0.45\n",
      "episode: 81/500, score: 89920.95512967452, e: 0.44\n",
      "episode: 82/500, score: 63611.079267457084, e: 0.44\n",
      "episode: 83/500, score: 38432.201101215665, e: 0.43\n",
      "episode: 84/500, score: 175778.99744791113, e: 0.43\n",
      "episode: 85/500, score: 64107.54250656091, e: 0.43\n",
      "episode: 86/500, score: 266026.787331132, e: 0.42\n",
      "episode: 87/500, score: 1446300.3697049585, e: 0.42\n",
      "episode: 88/500, score: 31535.596042941295, e: 0.41\n",
      "episode: 89/500, score: 217341.47467182067, e: 0.41\n",
      "episode: 90/500, score: 49203.68846126533, e: 0.4\n",
      "episode: 91/500, score: 177575.30509998457, e: 0.4\n",
      "episode: 92/500, score: 121134.63152317505, e: 0.4\n",
      "episode: 93/500, score: 20770.959476693006, e: 0.39\n",
      "episode: 94/500, score: 51251.56880285922, e: 0.39\n",
      "episode: 95/500, score: 93481.83981342212, e: 0.38\n",
      "episode: 96/500, score: 199689.3858630973, e: 0.38\n",
      "episode: 97/500, score: 68323.84448825954, e: 0.38\n",
      "episode: 98/500, score: 60908.29571190705, e: 0.37\n",
      "episode: 99/500, score: 120442.71135033267, e: 0.37\n",
      "episode: 100/500, score: 238749.79623317652, e: 0.37\n",
      "episode: 101/500, score: 786971.603751629, e: 0.36\n",
      "episode: 102/500, score: 50397.04657534201, e: 0.36\n",
      "episode: 103/500, score: 46455.38993453947, e: 0.36\n",
      "episode: 104/500, score: 88502.98615816969, e: 0.35\n",
      "episode: 105/500, score: 110326.0014503812, e: 0.35\n",
      "episode: 106/500, score: 129030.26620412394, e: 0.34\n",
      "episode: 107/500, score: 20828.0877245714, e: 0.34\n",
      "episode: 108/500, score: 57222.27720824157, e: 0.34\n",
      "episode: 109/500, score: 1480525.1011011454, e: 0.33\n",
      "episode: 110/500, score: 666769.680555354, e: 0.33\n",
      "episode: 111/500, score: 61448.380771391094, e: 0.33\n",
      "episode: 112/500, score: 80003.39333969905, e: 0.32\n",
      "episode: 113/500, score: 50726.52044292742, e: 0.32\n",
      "episode: 114/500, score: 50672.80590645552, e: 0.32\n",
      "episode: 115/500, score: 144932.64351148345, e: 0.31\n",
      "episode: 116/500, score: 42039.779504276274, e: 0.31\n",
      "episode: 117/500, score: 100768.49652936631, e: 0.31\n",
      "episode: 118/500, score: 144959.62283225678, e: 0.31\n",
      "episode: 119/500, score: 234670.47084927326, e: 0.3\n",
      "episode: 120/500, score: 13518.212976111452, e: 0.3\n",
      "episode: 121/500, score: 130347.42898363905, e: 0.3\n",
      "episode: 122/500, score: 304343.7335638409, e: 0.29\n",
      "episode: 123/500, score: 42327.37724034594, e: 0.29\n",
      "episode: 124/500, score: 46255.80024435419, e: 0.29\n",
      "episode: 125/500, score: 84352.97761997822, e: 0.28\n",
      "episode: 126/500, score: 70588.29776983561, e: 0.28\n",
      "episode: 127/500, score: 130234.43472292768, e: 0.28\n",
      "episode: 128/500, score: 158200.30865799217, e: 0.28\n",
      "episode: 129/500, score: 505319.5024754231, e: 0.27\n",
      "episode: 130/500, score: 21449.82182948621, e: 0.27\n",
      "episode: 131/500, score: 481828.15786734363, e: 0.27\n",
      "episode: 132/500, score: 84128.28166233207, e: 0.27\n",
      "episode: 133/500, score: 102644.79238204959, e: 0.26\n",
      "episode: 134/500, score: 54899.84765422653, e: 0.26\n",
      "episode: 135/500, score: 112989.41996588255, e: 0.26\n",
      "episode: 136/500, score: 82275.5604851279, e: 0.25\n",
      "episode: 137/500, score: 96972.26799586252, e: 0.25\n",
      "episode: 138/500, score: 297666.2746959393, e: 0.25\n",
      "episode: 139/500, score: 127785.57331420068, e: 0.25\n",
      "episode: 140/500, score: 65882.12695550143, e: 0.24\n",
      "episode: 141/500, score: 104565.75349472882, e: 0.24\n",
      "episode: 142/500, score: 203386.64893490454, e: 0.24\n",
      "episode: 143/500, score: 154427.2897246822, e: 0.24\n",
      "episode: 144/500, score: 68481.22349045538, e: 0.24\n",
      "episode: 145/500, score: 16773.461829897362, e: 0.23\n",
      "episode: 146/500, score: 6195.974319685102, e: 0.23\n",
      "episode: 147/500, score: 169497.0194000914, e: 0.23\n",
      "episode: 148/500, score: 574829.5966200344, e: 0.23\n",
      "episode: 149/500, score: 91645.36969952413, e: 0.22\n",
      "episode: 150/500, score: 267230.6760022405, e: 0.22\n",
      "episode: 151/500, score: 278685.0628528684, e: 0.22\n",
      "episode: 152/500, score: 649930.4990001515, e: 0.22\n",
      "episode: 153/500, score: 38140.43932095651, e: 0.21\n",
      "episode: 154/500, score: 229252.35261015277, e: 0.21\n",
      "episode: 155/500, score: 39199.302718817744, e: 0.21\n",
      "episode: 156/500, score: 1083486.765351073, e: 0.21\n",
      "episode: 157/500, score: 734529.4280975027, e: 0.21\n",
      "episode: 158/500, score: 84493.71971223678, e: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 159/500, score: 261023.3674608978, e: 0.2\n",
      "episode: 160/500, score: 860152.4168343699, e: 0.2\n",
      "episode: 161/500, score: 462702.7973840511, e: 0.2\n",
      "episode: 162/500, score: 475105.0054852209, e: 0.2\n",
      "episode: 163/500, score: 428799.7786401746, e: 0.19\n",
      "episode: 164/500, score: 53701.8534092534, e: 0.19\n",
      "episode: 165/500, score: 204734.9075348067, e: 0.19\n",
      "episode: 166/500, score: 580753.3555514412, e: 0.19\n",
      "episode: 167/500, score: 104723.26201759929, e: 0.19\n",
      "episode: 168/500, score: 275156.5224914343, e: 0.18\n",
      "episode: 169/500, score: 448946.9136717896, e: 0.18\n",
      "episode: 170/500, score: 417830.0480538676, e: 0.18\n",
      "episode: 171/500, score: 369097.5440674706, e: 0.18\n",
      "episode: 172/500, score: 698457.3068292165, e: 0.18\n",
      "episode: 173/500, score: 130646.96717403506, e: 0.18\n",
      "episode: 174/500, score: 629546.5471209997, e: 0.17\n",
      "episode: 175/500, score: 297745.88303540164, e: 0.17\n",
      "episode: 176/500, score: 113509.24713805925, e: 0.17\n",
      "episode: 177/500, score: 300504.80031979573, e: 0.17\n",
      "episode: 178/500, score: 151639.14021116772, e: 0.17\n",
      "episode: 179/500, score: 355682.64017208753, e: 0.17\n",
      "episode: 180/500, score: 946545.6252105846, e: 0.16\n",
      "episode: 181/500, score: 309496.7886150542, e: 0.16\n",
      "episode: 182/500, score: 84454.87457984433, e: 0.16\n",
      "episode: 183/500, score: 904670.1841880068, e: 0.16\n",
      "episode: 184/500, score: 322049.6980950717, e: 0.16\n",
      "episode: 185/500, score: 651073.2448154369, e: 0.16\n",
      "episode: 186/500, score: 158489.29273799405, e: 0.15\n",
      "episode: 187/500, score: 94950.53689011815, e: 0.15\n",
      "episode: 188/500, score: 41637.29666623932, e: 0.15\n",
      "episode: 189/500, score: 41638.24984785235, e: 0.15\n",
      "episode: 190/500, score: 116400.63319782086, e: 0.15\n",
      "episode: 191/500, score: 755753.9136304948, e: 0.15\n",
      "episode: 192/500, score: 964165.5629136091, e: 0.15\n",
      "episode: 193/500, score: 145506.92636527194, e: 0.14\n",
      "episode: 194/500, score: 165350.65084207797, e: 0.14\n",
      "episode: 195/500, score: 164908.09665617044, e: 0.14\n",
      "episode: 196/500, score: 614254.398882566, e: 0.14\n",
      "episode: 197/500, score: 142294.35208926257, e: 0.14\n",
      "episode: 198/500, score: 228781.66852288708, e: 0.14\n",
      "episode: 199/500, score: 627954.9734571209, e: 0.14\n",
      "episode: 200/500, score: 56430.11222300099, e: 0.13\n",
      "episode: 201/500, score: 353750.44389814103, e: 0.13\n",
      "episode: 202/500, score: 619467.4647225786, e: 0.13\n",
      "episode: 203/500, score: 232680.23494931625, e: 0.13\n",
      "episode: 204/500, score: 106739.5911921672, e: 0.13\n",
      "episode: 205/500, score: 1459724.6537245908, e: 0.13\n",
      "episode: 206/500, score: 365746.0653119361, e: 0.13\n",
      "episode: 207/500, score: 468131.5436058403, e: 0.12\n",
      "episode: 208/500, score: 51665.11537179132, e: 0.12\n",
      "episode: 209/500, score: 101070.20044263145, e: 0.12\n",
      "episode: 210/500, score: 425054.17088381876, e: 0.12\n",
      "episode: 211/500, score: 200316.66099719013, e: 0.12\n",
      "episode: 212/500, score: 121495.22055769477, e: 0.12\n",
      "episode: 213/500, score: 89790.89626342531, e: 0.12\n",
      "episode: 214/500, score: 288455.7797717676, e: 0.12\n",
      "episode: 215/500, score: 531600.6089985578, e: 0.12\n",
      "episode: 216/500, score: 1057351.962956795, e: 0.11\n",
      "episode: 217/500, score: 534219.1987501737, e: 0.11\n",
      "episode: 218/500, score: 266311.02891478676, e: 0.11\n",
      "episode: 219/500, score: 306781.9547163804, e: 0.11\n",
      "episode: 220/500, score: 897658.4151491937, e: 0.11\n",
      "episode: 221/500, score: 503840.57426964585, e: 0.11\n",
      "episode: 222/500, score: 379367.49517844175, e: 0.11\n",
      "episode: 223/500, score: 168323.5885652822, e: 0.11\n",
      "episode: 224/500, score: 212272.15108601245, e: 0.11\n",
      "episode: 225/500, score: 248968.11618652177, e: 0.1\n",
      "episode: 226/500, score: 154663.5884856916, e: 0.1\n",
      "episode: 227/500, score: 431670.0945513064, e: 0.1\n",
      "episode: 228/500, score: 117652.37389511056, e: 0.1\n",
      "episode: 229/500, score: 223641.26510875855, e: 0.1\n",
      "episode: 230/500, score: 324309.6853688175, e: 0.099\n",
      "episode: 231/500, score: 1618023.8288498872, e: 0.098\n",
      "episode: 232/500, score: 2782510.973844359, e: 0.097\n",
      "episode: 233/500, score: 341617.9491193984, e: 0.096\n",
      "episode: 234/500, score: 310289.432391162, e: 0.095\n",
      "episode: 235/500, score: 599804.1768552249, e: 0.094\n",
      "episode: 236/500, score: 238883.86590544312, e: 0.093\n",
      "episode: 237/500, score: 159776.24425421722, e: 0.092\n",
      "episode: 238/500, score: 864074.580873063, e: 0.091\n",
      "episode: 239/500, score: 244532.90651117405, e: 0.091\n",
      "episode: 240/500, score: 265473.65147304157, e: 0.09\n",
      "episode: 241/500, score: 146934.6567965568, e: 0.089\n",
      "episode: 242/500, score: 311493.9428092031, e: 0.088\n",
      "episode: 243/500, score: 1153950.8161737835, e: 0.087\n",
      "episode: 244/500, score: 632060.3893651449, e: 0.086\n",
      "episode: 245/500, score: 59984.630835710734, e: 0.085\n",
      "episode: 246/500, score: 208634.1414451522, e: 0.084\n",
      "episode: 247/500, score: 633009.9747504072, e: 0.084\n",
      "episode: 248/500, score: 118402.36508996657, e: 0.083\n",
      "episode: 249/500, score: 223478.54079644094, e: 0.082\n",
      "episode: 250/500, score: 84246.48136150982, e: 0.081\n",
      "episode: 251/500, score: 544015.3140723256, e: 0.08\n",
      "episode: 252/500, score: 896846.4014863672, e: 0.079\n",
      "episode: 253/500, score: 107303.82447112298, e: 0.079\n",
      "episode: 254/500, score: 786409.4598584528, e: 0.078\n",
      "episode: 255/500, score: 477830.386139111, e: 0.077\n",
      "episode: 256/500, score: 449515.4862400941, e: 0.076\n",
      "episode: 257/500, score: 733634.5279907815, e: 0.076\n",
      "episode: 258/500, score: 478308.451097368, e: 0.075\n",
      "episode: 259/500, score: 567184.0639412439, e: 0.074\n",
      "episode: 260/500, score: 294900.6872544726, e: 0.073\n",
      "episode: 261/500, score: 130820.00652021158, e: 0.073\n",
      "episode: 262/500, score: 827170.3486680076, e: 0.072\n",
      "episode: 263/500, score: 280446.6212336966, e: 0.071\n",
      "episode: 264/500, score: 368443.15139771707, e: 0.07\n",
      "episode: 265/500, score: 347913.85698605014, e: 0.07\n",
      "episode: 266/500, score: 256712.20883486114, e: 0.069\n",
      "episode: 267/500, score: 204048.4402660137, e: 0.068\n",
      "episode: 268/500, score: 641674.5689332274, e: 0.068\n",
      "episode: 269/500, score: 592411.720212682, e: 0.067\n",
      "episode: 270/500, score: 23221.728054333566, e: 0.066\n",
      "episode: 271/500, score: 126460.06005432496, e: 0.066\n",
      "episode: 272/500, score: 187650.86771984526, e: 0.065\n",
      "episode: 273/500, score: 595961.7730635034, e: 0.064\n",
      "episode: 274/500, score: 1813544.0536414925, e: 0.064\n",
      "episode: 275/500, score: 299042.83144301904, e: 0.063\n",
      "episode: 276/500, score: 122016.78413566289, e: 0.062\n",
      "episode: 277/500, score: 796745.1129190541, e: 0.062\n",
      "episode: 278/500, score: 131839.1308390198, e: 0.061\n",
      "episode: 279/500, score: 935780.0227823781, e: 0.061\n",
      "episode: 280/500, score: 152427.0711525774, e: 0.06\n",
      "episode: 281/500, score: 121040.58392818524, e: 0.059\n",
      "episode: 282/500, score: 508197.6733787904, e: 0.059\n",
      "episode: 283/500, score: 619094.9384361667, e: 0.058\n",
      "episode: 284/500, score: 873771.554169109, e: 0.058\n",
      "episode: 285/500, score: 631622.0366193892, e: 0.057\n",
      "episode: 286/500, score: 363929.63741398783, e: 0.056\n",
      "episode: 287/500, score: 403090.7285707559, e: 0.056\n",
      "episode: 288/500, score: 384003.8326763173, e: 0.055\n",
      "episode: 289/500, score: 285279.3303856732, e: 0.055\n",
      "episode: 290/500, score: 597549.8923028011, e: 0.054\n",
      "episode: 291/500, score: 289563.7593266333, e: 0.054\n",
      "episode: 292/500, score: 36329.36759806449, e: 0.053\n",
      "episode: 293/500, score: 85089.34724065523, e: 0.053\n",
      "episode: 294/500, score: 347167.8692133657, e: 0.052\n",
      "episode: 295/500, score: 308668.6037548536, e: 0.052\n",
      "episode: 296/500, score: 510083.493136358, e: 0.051\n",
      "episode: 297/500, score: 388255.75481140526, e: 0.051\n",
      "episode: 298/500, score: 1180069.3755255467, e: 0.05\n",
      "episode: 299/500, score: 1349185.9858030262, e: 0.05\n",
      "episode: 300/500, score: 543981.9759967085, e: 0.05\n",
      "episode: 301/500, score: 154534.2835560067, e: 0.05\n",
      "episode: 302/500, score: 375451.1807721077, e: 0.05\n",
      "episode: 303/500, score: 391885.62968841544, e: 0.05\n",
      "episode: 304/500, score: 269998.5756016411, e: 0.05\n",
      "episode: 305/500, score: 225639.9290159384, e: 0.05\n",
      "episode: 306/500, score: 598145.0318001467, e: 0.05\n",
      "episode: 307/500, score: 205844.88039542493, e: 0.05\n",
      "episode: 308/500, score: 633080.7704225347, e: 0.05\n",
      "episode: 309/500, score: 457305.72780309187, e: 0.05\n",
      "episode: 310/500, score: 405062.1392632412, e: 0.05\n",
      "episode: 311/500, score: 128136.7790055653, e: 0.05\n",
      "episode: 312/500, score: 514510.0752615576, e: 0.05\n",
      "episode: 313/500, score: 625226.4111737002, e: 0.05\n",
      "episode: 314/500, score: 349723.036157167, e: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 315/500, score: 1187760.4687652008, e: 0.05\n",
      "episode: 316/500, score: 285276.335672143, e: 0.05\n",
      "episode: 317/500, score: 215441.2970557708, e: 0.05\n",
      "episode: 318/500, score: 167816.88326879463, e: 0.05\n",
      "episode: 319/500, score: 165745.43370293768, e: 0.05\n",
      "episode: 320/500, score: 404024.5872110892, e: 0.05\n",
      "episode: 321/500, score: 267481.20356724155, e: 0.05\n",
      "episode: 322/500, score: 572855.9063094258, e: 0.05\n",
      "episode: 323/500, score: 288254.3376071631, e: 0.05\n",
      "episode: 324/500, score: 375757.70695063897, e: 0.05\n",
      "episode: 325/500, score: 113399.56399551718, e: 0.05\n",
      "episode: 326/500, score: 1503533.360753903, e: 0.05\n",
      "episode: 327/500, score: 140413.41734068256, e: 0.05\n",
      "episode: 328/500, score: 119924.31122392294, e: 0.05\n",
      "episode: 329/500, score: 328036.110368658, e: 0.05\n",
      "episode: 330/500, score: 46924.02633364915, e: 0.05\n",
      "episode: 331/500, score: 1602524.514172385, e: 0.05\n",
      "episode: 332/500, score: 131324.43864739256, e: 0.05\n",
      "episode: 333/500, score: 217362.5247773279, e: 0.05\n",
      "episode: 334/500, score: 142969.35402907457, e: 0.05\n",
      "episode: 335/500, score: 1738274.3359758733, e: 0.05\n",
      "episode: 336/500, score: 536672.7933421188, e: 0.05\n",
      "episode: 337/500, score: 591226.4894730559, e: 0.05\n",
      "episode: 338/500, score: 181785.7257770557, e: 0.05\n",
      "episode: 339/500, score: 1467356.2998905182, e: 0.05\n",
      "episode: 340/500, score: 232982.2026735731, e: 0.05\n",
      "episode: 341/500, score: 294089.95247040025, e: 0.05\n",
      "episode: 342/500, score: 278091.43053294165, e: 0.05\n",
      "episode: 343/500, score: 428852.23240085813, e: 0.05\n",
      "episode: 344/500, score: 652698.5713783171, e: 0.05\n",
      "episode: 345/500, score: 735429.0252873948, e: 0.05\n",
      "episode: 346/500, score: 407046.15276540566, e: 0.05\n",
      "episode: 347/500, score: 186617.69672433447, e: 0.05\n",
      "episode: 348/500, score: 54767.760862029, e: 0.05\n",
      "episode: 349/500, score: 161441.99566407732, e: 0.05\n",
      "episode: 350/500, score: 389804.19788018044, e: 0.05\n",
      "episode: 351/500, score: 194212.90870428117, e: 0.05\n",
      "episode: 352/500, score: 139074.9924859406, e: 0.05\n",
      "episode: 353/500, score: 828094.0192586917, e: 0.05\n",
      "episode: 354/500, score: 207890.11851232333, e: 0.05\n",
      "episode: 355/500, score: 165496.01479982617, e: 0.05\n",
      "episode: 356/500, score: 147641.31470948, e: 0.05\n"
     ]
    }
   ],
   "source": [
    "max_reward = None\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = state.values.reshape([1, state_size])\n",
    "    while env.enough_data_provided():\n",
    "        action_idx = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action_idx) # build these parameters into the NN model\n",
    "        # reward = reward if not done else -10\n",
    "        next_state = next_state.values.reshape([1, state_size])\n",
    "        agent.remember(state, action_idx, reward, next_state, done)\n",
    "        state = next_state\n",
    "    \n",
    "    if max_reward is None or max_reward[0] < env.score():\n",
    "        max_reward = (env.score(), env.actions)\n",
    "    \n",
    "    agent.update_target_model()\n",
    "    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, env.score(), agent.epsilon))\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "print(max_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
