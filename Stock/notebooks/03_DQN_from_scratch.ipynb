{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randPair(s,e):\n",
    "    return np.random.randint(s,e), np.random.randint(s,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finds an array in the \"depth\" dimension of the grid\n",
    "def findLoc(state, obj):\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            if (state[i,j] == obj).all():\n",
    "                return i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.arange(0,16,1).reshape(4,4)\n",
    "findLoc(state, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "* Player\n",
    "* Wall\n",
    "* Pit\n",
    "* Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize stationary grid, all items are placed deterministically\n",
    "def initGrid():\n",
    "    state = np.zeros((4,4,4))\n",
    "    #place player\n",
    "    state[0,1,3] = 1\n",
    "    #place wall\n",
    "    state[2,2,2] = 1\n",
    "    #place pit\n",
    "    state[1,1,1] = 1\n",
    "    #place goal\n",
    "    state[3,3,0] = 1\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = initGrid()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize player in random location, but keep wall, goal and pit stationary\n",
    "def initGridPlayer():\n",
    "    state = np.zeros((4,4,4))\n",
    "    #place player\n",
    "    state[randPair(0,4)] = np.array([0,0,0,1])\n",
    "    #place wall\n",
    "    state[2,2] = np.array([0,0,1,0])\n",
    "    #place pit\n",
    "    state[1,1] = np.array([0,1,0,0])\n",
    "    #place goal\n",
    "    state[1,2] = np.array([1,0,0,0])\n",
    "    \n",
    "    a = findLoc(state, np.array([0,0,0,1])) #find grid position of player (agent)\n",
    "    w = findLoc(state, np.array([0,0,1,0])) #find wall\n",
    "    g = findLoc(state, np.array([1,0,0,0])) #find goal\n",
    "    p = findLoc(state, np.array([0,1,0,0])) #find pit\n",
    "    if (not a or not w or not g or not p):\n",
    "        #print('Invalid grid. Rebuilding..')\n",
    "        return initGridPlayer()\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize grid so that goal, pit, wall, player are all randomly placed\n",
    "def initGridRand():\n",
    "    state = np.zeros((4,4,4))\n",
    "    #place player\n",
    "    state[randPair(0,4)] = np.array([0,0,0,1])\n",
    "    #place wall\n",
    "    state[randPair(0,4)] = np.array([0,0,1,0])\n",
    "    #place pit\n",
    "    state[randPair(0,4)] = np.array([0,1,0,0])\n",
    "    #place goal\n",
    "    state[randPair(0,4)] = np.array([1,0,0,0])\n",
    "    \n",
    "    a = findLoc(state, np.array([0,0,0,1]))\n",
    "    w = findLoc(state, np.array([0,0,1,0]))\n",
    "    g = findLoc(state, np.array([1,0,0,0]))\n",
    "    p = findLoc(state, np.array([0,1,0,0]))\n",
    "    #If any of the \"objects\" are superimposed, just call the function again to re-place\n",
    "    if (not a or not w or not g or not p):\n",
    "        #print('Invalid grid. Rebuilding..')\n",
    "        return initGridRand()\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeMove(state, action):\n",
    "    #need to locate player in grid\n",
    "    #need to determine what object (if any) is in the new grid spot the player is moving to\n",
    "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
    "    wall = findLoc(state, np.array([0,0,1,0]))\n",
    "    goal = findLoc(state, np.array([1,0,0,0]))\n",
    "    pit = findLoc(state, np.array([0,1,0,0]))\n",
    "    state = np.zeros((4,4,4))\n",
    "\n",
    "    actions = [[-1,0],[1,0],[0,-1],[0,1]]\n",
    "    #e.g. up => (player row - 1, player column + 0)\n",
    "    new_loc = (player_loc[0] + actions[action][0], player_loc[1] + actions[action][1])\n",
    "    if (new_loc != wall):\n",
    "        if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
    "            state[new_loc][3] = 1\n",
    "\n",
    "    new_player_loc = findLoc(state, np.array([0,0,0,1]))\n",
    "    if (not new_player_loc):\n",
    "        state[player_loc] = np.array([0,0,0,1])\n",
    "    #re-place pit\n",
    "    state[pit][1] = 1\n",
    "    #re-place wall\n",
    "    state[wall][2] = 1\n",
    "    #re-place goal\n",
    "    state[goal][0] = 1\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLoc(state, level):\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            if (state[i,j][level] == 1):\n",
    "                return i,j\n",
    "\n",
    "def getReward(state):\n",
    "    player_loc = getLoc(state, 3)\n",
    "    pit = getLoc(state, 1)\n",
    "    goal = getLoc(state, 0)\n",
    "    if (player_loc == pit):\n",
    "        return -10\n",
    "    elif (player_loc == goal):\n",
    "        return 10\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def dispGrid(state):\n",
    "    grid = np.zeros((4,4), dtype=np.str)\n",
    "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
    "    wall = findLoc(state, np.array([0,0,1,0]))\n",
    "    goal = findLoc(state, np.array([1,0,0,0]))\n",
    "    pit = findLoc(state, np.array([0,1,0,0]))\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            grid[i,j] = ' '\n",
    "            \n",
    "    if player_loc:\n",
    "        grid[player_loc] = 'P' #player\n",
    "    if wall:\n",
    "        grid[wall] = 'W' #wall\n",
    "    if goal:\n",
    "        grid[goal] = '+' #goal\n",
    "    if pit:\n",
    "        grid[pit] = '-' #pit\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[' ', ' ', ' ', ' '],\n",
       "       ['+', ' ', 'W', ' '],\n",
       "       [' ', ' ', ' ', '-'],\n",
       "       [' ', 'P', ' ', ' ']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = initGridRand()\n",
    "dispGrid(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -1\n",
      "Reward: -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[' ', ' ', ' ', ' '],\n",
       "       ['+', ' ', 'W', ' '],\n",
       "       [' ', ' ', ' ', '-'],\n",
       "       [' ', ' ', 'P', ' ']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = makeMove(state, 1)\n",
    "print('Reward: %s' % (getReward(state),))\n",
    "state = makeMove(state, 3)\n",
    "print('Reward: %s' % (getReward(state),))\n",
    "dispGrid(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the Q function\n",
    "NN with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(164, input_shape=(64,)))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2)) I'm not using dropout, but maybe you wanna give it a try?\n",
    "\n",
    "model.add(Dense(150))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10892771, -0.1549127 ,  0.04893943, -0.10359161]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(state.reshape(1,64), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Training\n",
    "\n",
    "* Setup a for-loop to number of epochs\n",
    "* In the loop, setup while loop (while game is in progress)\n",
    "* Run Q network **forward**, to get the **target output vector (TOV)**.\n",
    "* We're using an **epsilon greedy** implementation, so at time t with probability ϵ we will choose a **random action**. With probability 1−ϵ we will choose the action associated with the **highest Q value** from our neural network.\n",
    "* Take action a' as determined previously, observe new state s' and **reward $r_{t+1}$**\n",
    "* Run the network **forward** using s'. Store the **highest Q value (maxQ)**.\n",
    "* Our target value to train the network is **reward + (gamma * maxQ)** where gamma is a parameter (0<=γ<=1).\n",
    "* Given that we have **4 outputs** and we only want to update/train the output associated with the action we just took, **our TOV** is the same, except **we change the one output associated with our action to: reward + (gamma * maxQ)**\n",
    "* Train the model on this 1 sample. Repeat process 2-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 999\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s - loss: 0.0084\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "epochs = 1000\n",
    "gamma = 0.9 #since it may take several moves to goal, making gamma high\n",
    "epsilon = 1\n",
    "for i in range(epochs):\n",
    "    \n",
    "    state = initGrid()\n",
    "    status = 1\n",
    "    #while game still in progress\n",
    "    while(status == 1):\n",
    "        #We are in state S\n",
    "        #Let's run our Q function on S to get Q values for all possible actions\n",
    "        qval = model.predict(state.reshape(1,64), batch_size=1)\n",
    "        if (random.random() < epsilon): #choose random action\n",
    "            action = np.random.randint(0,4)\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state = makeMove(state, action)\n",
    "        #Observe reward\n",
    "        reward = getReward(new_state)\n",
    "        #Get max_Q(S',a)\n",
    "        newQ = model.predict(new_state.reshape(1,64), batch_size=1)\n",
    "        maxQ = np.max(newQ)\n",
    "        y = np.zeros((1,4))\n",
    "        y[:] = qval[:]\n",
    "        if reward == -1: #non-terminal state (-1)\n",
    "            update = (reward + (gamma * maxQ))\n",
    "        else: #terminal state (+10 or -10)\n",
    "            update = reward\n",
    "        y[0][action] = update #target output\n",
    "        print(\"Game #: %s\" % (i,))\n",
    "        model.fit(state.reshape(1,64), y, batch_size=1, epochs=1, verbose=1)\n",
    "        state = new_state\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "        clear_output(wait=True)\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testAlgo(init=0):\n",
    "    i = 0\n",
    "    if init==0:\n",
    "        state = initGrid()\n",
    "    elif init==1:\n",
    "        state = initGridPlayer()\n",
    "    elif init==2:\n",
    "        state = initGridRand()\n",
    "\n",
    "    print(\"Initial State:\")\n",
    "    print(dispGrid(state))\n",
    "    status = 1\n",
    "    #while game still in progress\n",
    "    while(status == 1):\n",
    "        qval = model.predict(state.reshape(1,64), batch_size=1)\n",
    "        action = (np.argmax(qval)) #take action with highest Q-value\n",
    "        print('Move #: %s; Taking action: %s' % (i, action))\n",
    "        state = makeMove(state, action)\n",
    "        print(dispGrid(state))\n",
    "        reward = getReward(state)\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "            print(\"Reward: %s\" % (reward,))\n",
    "        i += 1 #If we're taking more than 10 actions, just stop, we probably can't win this game\n",
    "        if (i > 10):\n",
    "            print(\"Game lost; too many moves.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[' ' 'P' ' ' ' ']\n",
      " [' ' '-' ' ' ' ']\n",
      " [' ' ' ' 'W' ' ']\n",
      " [' ' ' ' ' ' '+']]\n",
      "Move #: 0; Taking action: 3\n",
      "[[' ' ' ' 'P' ' ']\n",
      " [' ' '-' ' ' ' ']\n",
      " [' ' ' ' 'W' ' ']\n",
      " [' ' ' ' ' ' '+']]\n",
      "Move #: 1; Taking action: 1\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' '-' 'P' ' ']\n",
      " [' ' ' ' 'W' ' ']\n",
      " [' ' ' ' ' ' '+']]\n",
      "Move #: 2; Taking action: 3\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' '-' ' ' 'P']\n",
      " [' ' ' ' 'W' ' ']\n",
      " [' ' ' ' ' ' '+']]\n",
      "Move #: 3; Taking action: 1\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' '-' ' ' ' ']\n",
      " [' ' ' ' 'W' 'P']\n",
      " [' ' ' ' ' ' '+']]\n",
      "Move #: 4; Taking action: 1\n",
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' '-' ' ' ' ']\n",
      " [' ' ' ' 'W' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "testAlgo(init=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic forgetting\n",
    "* Associated with gradient descent based training methods in online training\n",
    "* With a different initial state and after a very different reward: it may do backpropagation again to update its state-action value but because this state-action is very similar to the last learned state-action it may mess up its previously learned weights\n",
    "* There's a push-pull between very similar state-actions (but with divergent targets) that results in this inability to properly learn anything\n",
    "* Randomized batch learning in supervised learning\n",
    "* **Experience replay** in DQN: minibatch updating in an online learning scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "* In state s, take action a, observe new state $s_{t+1}$ and reward $r_{t+1}$\n",
    "* Store this as a tuple (s,a,$s_{t+1}$,$r_{t+1}$) in a list.\n",
    "* Continue to store each experience in this list until we have filled the list to a specific length (up to you to define)\n",
    "* Once the experience replay memory is filled, randomly select a subset (e.g. 40)\n",
    "* Iterate through this subset and calculate value updates for each; store these in a target array (e.g. y_train) and store the state s of each memory in X_train\n",
    "* Use X_train and y_train as a minibatch for batch training. For subsequent epochs where the array is full, just overwrite old values in our experience replay memory array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 1677\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s - loss: 5.4847e-06\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=adam)#reset weights of neural network\n",
    "epochs = 3000\n",
    "gamma = 0.975\n",
    "epsilon = 1\n",
    "batchSize = 40\n",
    "buffer = 80\n",
    "replay = []\n",
    "#stores tuples of (S, A, R, S')\n",
    "h = 0\n",
    "for i in range(epochs):\n",
    "    \n",
    "    state = initGridPlayer() #using the harder state initialization function\n",
    "    status = 1\n",
    "    #while game still in progress\n",
    "    while(status == 1):\n",
    "        #We are in state S\n",
    "        #Let's run our Q function on S to get Q values for all possible actions\n",
    "        qval = model.predict(state.reshape(1,64), batch_size=1)\n",
    "        if (random.random() < epsilon): #choose random action\n",
    "            action = np.random.randint(0,4)\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state = makeMove(state, action)\n",
    "        #Observe reward\n",
    "        reward = getReward(new_state)\n",
    "        \n",
    "        #Experience replay storage\n",
    "        if (len(replay) < buffer): #if buffer not filled, add to it\n",
    "            replay.append((state, action, reward, new_state))\n",
    "        else: #if buffer full, overwrite old values\n",
    "            if (h < (buffer-1)):\n",
    "                h += 1\n",
    "            else:\n",
    "                h = 0\n",
    "            replay[h] = (state, action, reward, new_state)\n",
    "            #randomly sample our experience replay memory\n",
    "            minibatch = random.sample(replay, batchSize)\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            for memory in minibatch:\n",
    "                #Get max_Q(S',a)\n",
    "                old_state, action, reward, new_state = memory\n",
    "                old_qval = model.predict(old_state.reshape(1,64), batch_size=1)\n",
    "                newQ = model.predict(new_state.reshape(1,64), batch_size=1)\n",
    "                maxQ = np.max(newQ)\n",
    "                y = np.zeros((1,4))\n",
    "                y[:] = old_qval[:]\n",
    "                if reward == -1: #non-terminal state\n",
    "                    update = (reward + (gamma * maxQ))\n",
    "                else: #terminal state\n",
    "                    update = reward\n",
    "                y[0][action] = update\n",
    "                X_train.append(old_state.reshape(64,))\n",
    "                y_train.append(y.reshape(4,))\n",
    "            \n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            print(\"Game #: %s\" % (i,))\n",
    "            model.fit(X_train, y_train, batch_size=batchSize, nb_epoch=1, verbose=1)\n",
    "            state = new_state\n",
    "        if reward != -1: #if reached terminal state, update game status\n",
    "            status = 0\n",
    "        clear_output(wait=True)\n",
    "    if epsilon > 0.1: #decrement epsilon over time\n",
    "        epsilon -= (1/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testAlgo(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testAlgo(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
